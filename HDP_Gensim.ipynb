{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a68bf4",
   "metadata": {},
   "source": [
    "## HDP Model with Genesim\n",
    "### 1. Corpus Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e8e7a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.17690613756060086),\n",
      " (1, 0.13205008895998877),\n",
      " (2, 0.14336469175790142),\n",
      " (3, 0.14913808879720644),\n",
      " (4, 0.18731716171680116),\n",
      " (5, 0.19524770945331216),\n",
      " (6, 0.04689342344440567),\n",
      " (7, 0.26331130930551977),\n",
      " (8, 0.26416774953322836),\n",
      " (9, 0.19340339809371676),\n",
      " (10, 0.31448092903974306),\n",
      " (11, 0.3189738583094442),\n",
      " (12, 0.096278199935693),\n",
      " (13, 0.10405330187201527),\n",
      " (14, 0.17849338528617428),\n",
      " (15, 0.06802944545411793),\n",
      " (16, 0.043326972794528074),\n",
      " (17, 0.08654357014989379),\n",
      " (18, 0.3695068229344004),\n",
      " (19, 0.37115918582026985),\n",
      " (20, 0.11042566660647649),\n",
      " (21, 0.20566964840912783),\n",
      " (22, 0.13371304691656322),\n",
      " (23, 0.208036474980719)]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve data from Preprocessing\n",
    "%store -r data_lemmatized df\n",
    "\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = gensim.corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Filtering out of tokens appearing in less than 20 documents or more than 70% of documents\n",
    "id2word.filter_extremes(no_below=20, no_above=0.7)\n",
    "\n",
    "# Create Corpus\n",
    "bow_corpus = [id2word.doc2bow(doc) for doc in data_lemmatized]\n",
    "\n",
    "# Create the TF-IDF model based on the bag-of-words corpus\n",
    "tfidf_model = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# Apply the TF-IDF transformation to the corpus\n",
    "tfidf_corpus = tfidf_model[bow_corpus]\n",
    "\n",
    "for doc in tfidf_corpus:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62025fe5",
   "metadata": {},
   "source": [
    "### 2. Defauld HDP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "874a34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel\n",
    "\n",
    "# calculate hdp model with default parameters\n",
    "hdp_model = HdpModel(tfidf_corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b35c1c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*canal + 0.003*decent + 0.003*attack + 0.003*freeze + 0.003*labor + 0.002*ready + 0.002*shoulder + 0.002*perhaps + 0.002*heap + 0.002*back + 0.002*join + 0.002*heaven + 0.002*sat + 0.002*combination + 0.002*target + 0.002*lifetime + 0.002*dress + 0.002*overcrowded + 0.002*nice + 0.002*test'),\n",
       " (1,\n",
       "  '0.003*hamburger + 0.002*lesson + 0.002*still + 0.002*differently + 0.002*animation + 0.002*par + 0.002*unhappy + 0.002*thoroughly + 0.002*planning + 0.002*reasonable + 0.002*seeker + 0.002*entirely + 0.002*pretty + 0.002*asap + 0.002*snack + 0.002*human + 0.002*picture + 0.002*dinosaur + 0.002*vader + 0.002*treatment'),\n",
       " (2,\n",
       "  '0.003*increase + 0.003*piece + 0.003*apart + 0.002*nightmare + 0.002*handicap + 0.002*lift + 0.002*elevator + 0.002*go + 0.002*color + 0.002*new + 0.002*dress + 0.002*traveller + 0.002*answer + 0.002*pricy + 0.002*passholders + 0.002*nostalgia + 0.002*warning + 0.002*firework + 0.002*greedy + 0.002*sweet'),\n",
       " (3,\n",
       "  '0.003*close + 0.003*tuesday + 0.003*sept + 0.002*mobile + 0.002*alcohol + 0.002*apps + 0.002*factor + 0.002*brother + 0.002*grizzly + 0.002*scare + 0.002*lake + 0.002*pro + 0.002*low + 0.002*everybody + 0.002*jam + 0.002*delightful + 0.002*decide + 0.002*discount + 0.002*language + 0.002*february'),\n",
       " (4,\n",
       "  '0.003*humid + 0.002*submarine + 0.002*bubble + 0.002*hatter + 0.002*willing + 0.002*cup + 0.002*half + 0.002*capture + 0.002*age + 0.002*too + 0.002*fun + 0.002*san + 0.002*forward + 0.002*elsa + 0.002*sour + 0.002*screaming + 0.002*beginning + 0.002*meaning + 0.002*anyone + 0.002*inn'),\n",
       " (5,\n",
       "  '0.004*clean + 0.003*possibility + 0.002*terribly + 0.002*necessary + 0.002*mon + 0.002*drinking + 0.002*boardwalk + 0.002*frequently + 0.002*fortunately + 0.002*weekend + 0.002*cooky + 0.002*fav + 0.002*vantage + 0.002*ease + 0.002*health + 0.002*absolutely + 0.002*accommodation + 0.002*grow + 0.002*dinosaur + 0.002*review'),\n",
       " (6,\n",
       "  '0.003*chair + 0.003*photograph + 0.003*age + 0.003*red + 0.003*central + 0.002*ever + 0.002*sugar + 0.002*midday + 0.002*thank + 0.002*haunted + 0.002*daughter + 0.002*prop + 0.002*dad + 0.002*crazy + 0.002*renew + 0.002*soundsational + 0.002*completely + 0.002*times + 0.002*ill + 0.002*rain'),\n",
       " (7,\n",
       "  '0.003*unhappy + 0.003*pooh + 0.003*outstanding + 0.003*rapid + 0.002*stressful + 0.002*scooter + 0.002*hat + 0.002*target + 0.002*loose + 0.002*right + 0.002*hang + 0.002*internet + 0.002*ambience + 0.002*pop + 0.002*absolutely + 0.002*look + 0.002*elsewhere + 0.002*utilise + 0.002*baby + 0.002*former'),\n",
       " (8,\n",
       "  '0.003*cave + 0.003*exit + 0.003*giant + 0.003*money + 0.002*alot + 0.002*opening + 0.002*incredibly + 0.002*log + 0.002*throw + 0.002*transport + 0.002*dramatically + 0.002*physically + 0.002*hill + 0.002*manage + 0.002*citizen + 0.002*conference + 0.002*bay + 0.002*woody + 0.002*souvenir + 0.002*transfer'),\n",
       " (9,\n",
       "  '0.003*switch + 0.002*name + 0.002*long + 0.002*outrageous + 0.002*unbelievable + 0.002*pretzel + 0.002*message + 0.002*user + 0.002*tight + 0.002*animation + 0.002*enjoyment + 0.002*over + 0.002*week + 0.002*weird + 0.002*minimize + 0.002*til + 0.002*ambiance + 0.002*senior + 0.002*greed + 0.002*improve'),\n",
       " (10,\n",
       "  '0.003*concern + 0.003*rip + 0.003*phenomenal + 0.003*express + 0.002*image + 0.002*lunch + 0.002*turkey + 0.002*efficiency + 0.002*management + 0.002*junk + 0.002*dry + 0.002*relative + 0.002*sooo + 0.002*claim + 0.002*over + 0.002*alcohol + 0.002*hair + 0.002*attendant + 0.002*slot + 0.002*park'),\n",
       " (11,\n",
       "  '0.003*continually + 0.002*blast + 0.002*caters + 0.002*appropriate + 0.002*obvious + 0.002*have + 0.002*nice + 0.002*rib + 0.002*juice + 0.002*malfunction + 0.002*many + 0.002*tripadvisor + 0.002*smile + 0.002*noticeable + 0.002*exhibit + 0.002*fun + 0.002*however + 0.002*breakdown + 0.002*park + 0.002*pretzel'),\n",
       " (12,\n",
       "  '0.003*walking + 0.003*etc + 0.003*appeal + 0.002*bypass + 0.002*old + 0.002*upkeep + 0.002*recommend + 0.002*combination + 0.002*woody + 0.002*catch + 0.002*bay + 0.002*answer + 0.002*obtain + 0.002*socal + 0.002*generally + 0.002*enjoyable + 0.002*soarin + 0.002*par + 0.002*terrific + 0.002*home'),\n",
       " (13,\n",
       "  '0.003*buzz + 0.003*daisy + 0.003*absolutely + 0.002*evident + 0.002*international + 0.002*describe + 0.002*normally + 0.002*loose + 0.002*height + 0.002*accord + 0.002*price + 0.002*rise + 0.002*others + 0.002*frame + 0.002*nightmare + 0.002*story + 0.002*superb + 0.002*fraction + 0.002*somehow + 0.002*luck'),\n",
       " (14,\n",
       "  '0.003*cute + 0.002*bottle + 0.002*assist + 0.002*immaculately + 0.002*outdoor + 0.002*maintain + 0.002*space + 0.002*now + 0.002*recommend + 0.002*feeling + 0.002*handle + 0.002*room + 0.002*sunny + 0.002*recommended + 0.002*red + 0.002*shopping + 0.002*twain + 0.002*healthy + 0.002*teacup + 0.002*entirely'),\n",
       " (15,\n",
       "  '0.004*apps + 0.003*interactive + 0.003*nut + 0.003*exist + 0.003*challenge + 0.003*comparison + 0.003*bear + 0.003*soar + 0.002*really + 0.002*mcdonalds + 0.002*way + 0.002*fruit + 0.002*cost + 0.002*kill + 0.002*terrace + 0.002*fanatic + 0.002*adrenaline + 0.002*fairly + 0.002*trip + 0.002*tune'),\n",
       " (16,\n",
       "  '0.003*jazz + 0.003*dine + 0.003*while + 0.003*everybody + 0.003*veggie + 0.002*trading + 0.002*save + 0.002*thankfully + 0.002*appreciate + 0.002*needle + 0.002*yell + 0.002*autograph + 0.002*amusement + 0.002*load + 0.002*other + 0.002*accessible + 0.002*yesterday + 0.002*surprisingly + 0.002*issue + 0.002*before'),\n",
       " (17,\n",
       "  '0.003*beach + 0.003*norm + 0.002*tarzan + 0.002*complaint + 0.002*computer + 0.002*dangerous + 0.002*etc + 0.002*outside + 0.002*stroller + 0.002*jack + 0.002*set + 0.002*individual + 0.002*side + 0.002*organization + 0.002*distance + 0.002*live + 0.002*approximately + 0.002*worth + 0.002*exhibit + 0.002*fresh'),\n",
       " (18,\n",
       "  '0.003*ice + 0.003*frontierland + 0.003*rocket + 0.002*important + 0.002*loud + 0.002*next + 0.002*draw + 0.002*lol + 0.002*play + 0.002*load + 0.002*nicely + 0.002*wallet + 0.002*shirt + 0.002*encourage + 0.002*windy + 0.002*reservation + 0.002*pooh + 0.002*easily + 0.002*slowly + 0.002*creativity'),\n",
       " (19,\n",
       "  '0.003*buck + 0.003*decent + 0.003*property + 0.003*help + 0.002*twist + 0.002*ques + 0.002*crowd + 0.002*unexpected + 0.002*ultimate + 0.002*popularity + 0.002*railway + 0.002*treehouse + 0.002*exist + 0.002*swim + 0.002*birthday + 0.002*sad + 0.002*good + 0.002*possibly + 0.002*altogether + 0.002*booth')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show initial topics\n",
    "hdp_model.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae725051",
   "metadata": {},
   "source": [
    "#### Default coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f3f6dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.48511618616782565\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score with default alpha and beta values\n",
    "coherence_model = CoherenceModel(model=hdp_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print('Coherence Score: ', coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e1db63",
   "metadata": {},
   "source": [
    "### 3. Compute model performance metrics\n",
    "#### Calculation of Coherence Score with varying Truncation Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae51d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for calculation of coherence values\n",
    "def compute_coherence_values(corpus, dictionary, model):\n",
    "    coherence_model = CoherenceModel(model=model, texts=data_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "    return coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89ec7b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training HDP Models: 100%|████████████████████████| 9/9 [04:16<00:00, 28.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Truncation Level</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Num_Topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40</td>\n",
       "      <td>0.514168</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.505223</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>0.504326</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>0.504101</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>0.497760</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.493940</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.492919</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35</td>\n",
       "      <td>0.473465</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45</td>\n",
       "      <td>0.472601</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Truncation Level  Coherence  Num_Topics\n",
       "7                40   0.514168          40\n",
       "3                20   0.505223          20\n",
       "5                30   0.504326          30\n",
       "2                15   0.504101          15\n",
       "4                25   0.497760          25\n",
       "1                10   0.493940          10\n",
       "0                 5   0.492919           5\n",
       "6                35   0.473465          35\n",
       "8                45   0.472601          45"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from gensim.models import HdpModel\n",
    "import pandas as pd\n",
    "\n",
    "# Create list with varying truncation levels between 5 and 50\n",
    "t_list = [t for t in range(5, 50, 5)]\n",
    "results = []\n",
    "\n",
    "# Define funtion for training hdp model with varying truncation level\n",
    "def train_hdp_model_t(t):\n",
    "    hdp_model = HdpModel(corpus=tfidf_corpus, id2word=id2word, T=t)\n",
    "    \n",
    "    # Compute coherence value for the trained model\n",
    "    cv = compute_coherence_values(corpus=tfidf_corpus, dictionary=id2word, model=hdp_model)\n",
    "    \n",
    "    # Get the number of topics generated by the model\n",
    "    num_topics = len(hdp_model.get_topics())\n",
    "    \n",
    "    # Store the result in a dictionary\n",
    "    result = {'Truncation Level': t, 'Coherence': cv, 'Num_Topics': num_topics}\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create a progress bar and a thread pool executor for concurrent execution\n",
    "with tqdm(total=len(t_list), desc=\"Training HDP Models\") as pbar, ThreadPoolExecutor() as executor:\n",
    "    # Submit tasks to the executor for each truncation level in t_list\n",
    "    futures = [executor.submit(train_hdp_model_t, t) for t in t_list]\n",
    "    \n",
    "    # Wait for the tasks to complete and retrieve the results\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        results.append(result)\n",
    "        pbar.update(1)\n",
    "\n",
    "# Create a pandas DataFrame from the results\n",
    "df2 = pd.DataFrame(results)\n",
    "\n",
    "# Sort the DataFrame based on the 'Coherence' column in descending order\n",
    "df2.sort_values('Coherence', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab2859",
   "metadata": {},
   "source": [
    "#### Calculation of Coherence Score with varying alpha, beta & gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39a0c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for training the model with different parameters and predefined truncation level\n",
    "def train_hdp_model_params(alpha, beta, gamma):\n",
    "    hdp_model = HdpModel(corpus=tfidf_corpus, id2word=id2word, alpha=alpha, eta=beta, gamma=gamma, T=30)\n",
    "    \n",
    "    # Compute coherence value for the trained model\n",
    "    cv = compute_coherence_values(corpus=tfidf_corpus, dictionary=id2word, model=hdp_model)\n",
    "    \n",
    "    # Get the number of topics generated by the model\n",
    "    num_topics = len(hdp_model.get_topics())\n",
    "    return cv, num_topics\n",
    "\n",
    "# define range for alpha, beta & gamma\n",
    "alpha_list = [0.01, 0.31, 0.61, 0.91]\n",
    "beta_list = [0.01, 0.31, 0.61, 0.91]\n",
    "gamma_list = [1,2,3]\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "# Iterate over alpha and gamma values\n",
    "for alpha in alpha_list:\n",
    "    for beta in beta_list:\n",
    "        for gamma in gamma_list:\n",
    "            # Train the HDP model\n",
    "            cv, num_topics = train_hdp_model_params(alpha, beta, gamma)\n",
    "\n",
    "            # Store the results\n",
    "            result = {'Alpha': alpha, 'Beta': beta, 'Gamma': gamma, 'Coherence': cv, 'Num_Topics': num_topics}\n",
    "            results.append(result)\n",
    "        \n",
    "# create data frame with results                \n",
    "df3 = pd.DataFrame(results)\n",
    "df3.to_csv('hdp_tuning_results_gensim.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d059e611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Gamma</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Num_Topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1</td>\n",
       "      <td>0.529403</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2</td>\n",
       "      <td>0.509548</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1</td>\n",
       "      <td>0.507113</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.61</td>\n",
       "      <td>2</td>\n",
       "      <td>0.506699</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>0.506229</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Alpha  Beta  Gamma  Coherence  Num_Topics\n",
       "45   0.91  0.91      1   0.529403          30\n",
       "34   0.61  0.91      2   0.509548          30\n",
       "30   0.61  0.61      1   0.507113          30\n",
       "43   0.91  0.61      2   0.506699          30\n",
       "26   0.61  0.01      3   0.506229          30"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the model_results from the CSV file\n",
    "model_results = pd.read_csv('hdp_tuning_results_gensim.csv')\n",
    "\n",
    "# display the metrics and coherence score\n",
    "metrics_df = model_results.sort_values('Coherence', ascending=False).head()   \n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6442f4f",
   "metadata": {},
   "source": [
    "#### Selection of the optimum alpha and beta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32ea8816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91 1 0.5294031868100869\n"
     ]
    }
   ],
   "source": [
    "# select alpha, beta & gamma with the highest coherence value from the dataframe\n",
    "alpha = metrics_df.iloc[0,1]\n",
    "beta = metrics_df.iloc[0,2]\n",
    "gamma = metrics_df.iloc[0,3]\n",
    "\n",
    "print(alpha, beta, gamma)\n",
    "    \n",
    "\n",
    "# calculate the hdp with the selected parameters\n",
    "best_hdp_model = HdpModel(corpus=tfidf_corpus, id2word=id2word, alpha=alpha, eta=beta, gamma=gamma, T=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd52baa0",
   "metadata": {},
   "source": [
    "#### Topics distribution across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f632ec65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>Num Documents</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1014</td>\n",
       "      <td>0.046535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1029</td>\n",
       "      <td>0.044604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1191</td>\n",
       "      <td>0.043173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>1057</td>\n",
       "      <td>0.043170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>945</td>\n",
       "      <td>0.043155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1051</td>\n",
       "      <td>0.042805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1108</td>\n",
       "      <td>0.042653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>949</td>\n",
       "      <td>0.042620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>771</td>\n",
       "      <td>0.042584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1233</td>\n",
       "      <td>0.042273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1359</td>\n",
       "      <td>0.042219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1743</td>\n",
       "      <td>0.042084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1160</td>\n",
       "      <td>0.041685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>941</td>\n",
       "      <td>0.041484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1185</td>\n",
       "      <td>0.041477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1094</td>\n",
       "      <td>0.041424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>884</td>\n",
       "      <td>0.041387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>903</td>\n",
       "      <td>0.041015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1237</td>\n",
       "      <td>0.040888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>869</td>\n",
       "      <td>0.040660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>936</td>\n",
       "      <td>0.040422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>681</td>\n",
       "      <td>0.040298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>794</td>\n",
       "      <td>0.040266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>678</td>\n",
       "      <td>0.039915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>739</td>\n",
       "      <td>0.039877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>992</td>\n",
       "      <td>0.039241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>720</td>\n",
       "      <td>0.038869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>839</td>\n",
       "      <td>0.038434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1249</td>\n",
       "      <td>0.038304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>780</td>\n",
       "      <td>0.037195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_id  Num Documents    weight\n",
       "27        27           1014  0.046535\n",
       "28        28           1029  0.044604\n",
       "18        18           1191  0.043173\n",
       "25        25           1057  0.043170\n",
       "1          1            945  0.043155\n",
       "17        17           1051  0.042805\n",
       "7          7           1108  0.042653\n",
       "2          2            949  0.042620\n",
       "15        15            771  0.042584\n",
       "4          4           1233  0.042273\n",
       "26        26           1359  0.042219\n",
       "6          6           1743  0.042084\n",
       "14        14           1160  0.041685\n",
       "20        20            941  0.041484\n",
       "11        11           1185  0.041477\n",
       "29        29           1094  0.041424\n",
       "16        16            884  0.041387\n",
       "13        13            903  0.041015\n",
       "12        12           1237  0.040888\n",
       "24        24            869  0.040660\n",
       "10        10            936  0.040422\n",
       "3          3            681  0.040298\n",
       "0          0            794  0.040266\n",
       "5          5            678  0.039915\n",
       "22        22            739  0.039877\n",
       "21        21            992  0.039241\n",
       "23        23            720  0.038869\n",
       "9          9            839  0.038434\n",
       "19        19           1249  0.038304\n",
       "8          8            780  0.037195"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define function for extracting the topic probabilities and weights from the HDP model\n",
    "def topic_prob_extractor(gensim_hdp):   \n",
    "    shown_topics = gensim_hdp.show_topics(num_topics=30, formatted=False)\n",
    "    topics_nos = [x[0] for x in shown_topics]\n",
    "    weights = [sum([item[1] for item in x[1]]) for x in shown_topics]\n",
    "\n",
    "    return pd.DataFrame({'topic_id': topics_nos, 'weight': weights})\n",
    "\n",
    "# Retrieve the document-topic assignments\n",
    "topic_assignments = best_hdp_model[tfidf_corpus]\n",
    "\n",
    "# Count the number of documents assigned to each topic\n",
    "topic_counts = {}\n",
    "for doc_topics in topic_assignments:\n",
    "    for topic_id, topic_prob in doc_topics:\n",
    "        topic_counts[topic_id] = topic_counts.get(topic_id, 0) + 1\n",
    "\n",
    "# Create the DataFrame with topic counts and weights\n",
    "df_document_topic = pd.DataFrame({'Num Documents': topic_counts}).sort_values(by='Num Documents', ascending=False)\n",
    "\n",
    "# Calculate the topic weights\n",
    "df_topic_weights = topic_prob_extractor(best_hdp_model)\n",
    "\n",
    "# Merge the topic weights into the DataFrame\n",
    "df_document_topic = df_document_topic.merge(df_topic_weights, left_index=True, right_on='topic_id')\n",
    "df_document_topic = df_document_topic[['topic_id', 'Num Documents', 'weight']]\n",
    "\n",
    "# Sort the DataFrame by weights in descending order\n",
    "df_document_topic = df_document_topic.sort_values(by='weight', ascending=False)\n",
    "\n",
    "# Calculate the topic distribution\n",
    "df_topic_distribution = df_document_topic['topic_id'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "\n",
    "# Display the DataFrame\n",
    "df_document_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3a1b1e",
   "metadata": {},
   "source": [
    "#### Intertopic Distance Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import warnings\n",
    "\n",
    "# disable deprecation warning\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(best_hdp_model, tfidf_corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f99e9",
   "metadata": {},
   "source": [
    "![Inter-topic Distance Map](Topic_Distance_Maps/Intertopic_Map_HDP_Gensim.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2f3fe",
   "metadata": {},
   "source": [
    "### 4. Final Results\n",
    "#### Top 10 words assigned to each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447fd8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 10 keywords for each topic\n",
    "def show_topics(hdp_model, n_words=20):\n",
    "    topic_keywords = []\n",
    "    for topic_weights in hdp_model.get_topics():\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append([id2word[idx] for idx in top_keyword_locs])\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(hdp_model=best_hdp_model, n_words=10)        \n",
    "\n",
    "# Topic-Keywords DataFrame\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i+1) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i+1) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf45546",
   "metadata": {},
   "source": [
    "#### Assignment of the reviews to the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee449d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set column width to maximum to see the whole review\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Reset index of the DataFrame for matching\n",
    "df = df.reset_index(drop=True)\n",
    "df_document_topic = df_document_topic.reset_index(drop=True)\n",
    "\n",
    "# Join the dataset with the 'topic_id' column from df_document_topic\n",
    "df_joined = pd.merge(df, df_document_topic['topic_id'], left_index=True, right_index=True)\n",
    "\n",
    "df_joined.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
